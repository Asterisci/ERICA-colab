{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERICA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1x0ZQF8KZqSczzLCEt3xXkbpWLWmzOgBW",
      "authorship_tag": "ABX9TyPBg1rJnjCFyCz64y5ABLr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaoers/ERICA/blob/main/ERICA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg2rgTlD29dz",
        "outputId": "29747a1c-e6e9-4de4-f29a-45727abc7ca0"
      },
      "source": [
        "! git clone https://github.com/thunlp/ERICA.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ERICA' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Dn4ZH9Zze5",
        "outputId": "2ad9b5f8-018d-42bb-8973-3b5b497dc555"
      },
      "source": [
        "! mv /content/ERICA /content/drive/MyDrive"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2NCIqgrYQfI"
      },
      "source": [
        "! cd /content/ERICA/pretrain/data/DOC; rm -rf roberta_data.tar"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiRFU34OZUo5"
      },
      "source": [
        "! rm -rf /content/drive/MyDrive/ERICA"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_OTDuaV7NuX",
        "outputId": "561e6ec6-ca98-439d-83c5-8fbd49e98d8b"
      },
      "source": [
        "! pip install folium==0.2.1;pip install transformers==2.5.0;pip install pytorch_metric_learning;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting folium==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-cp37-none-any.whl size=79810 sha256=2b653bffc7f4ca979b09bdf81cc7caaa60c199b60458a14d1b233bc1bb50b0f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting transformers==2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/58/3d789b98923da6485f376be1e04d59ad7003a63bdb2b04b5eea7e02857e5/transformers-2.5.0-py3-none-any.whl (481kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/58/a229b18e4b10d14fc5df7e0d4227e04a81ef43a6fe22d1e53d9d81c5ca9d/tokenizers-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 26.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.0) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c0/8af2139d5658eccde11f45fd9d27046edb286fd60f5371e27870612287bd/boto3-1.17.111-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.0) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.0) (1.15.0)\n",
            "Collecting botocore<1.21.0,>=1.20.111\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/56/64570ac92c7cb88ad731dea4da4a83d3edc9f00a13a969ad826354ba5a58/botocore-1.20.111.tar.gz (7.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9MB 29.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.111->boto3->transformers==2.5.0) (2.8.1)\n",
            "Building wheels for collected packages: botocore\n",
            "  Building wheel for botocore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botocore: filename=botocore-1.20.111-py2.py3-none-any.whl size=7695032 sha256=10c7f89e485b6edd377632c630289bb91b9618bb70d1b7a524f9f738d1e4071b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/5f/dd/9021b3f78dc76c95f97ea9cd1798aa6da9bc1a61fe7d1bb9fa\n",
            "Successfully built botocore\n",
            "\u001b[31mERROR: botocore 1.20.111 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.17.111 botocore-1.20.111 jmespath-0.10.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.5.0 transformers-2.5.0\n",
            "Collecting pytorch_metric_learning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/1b/d85136231f8d14fdadcd0a13359bbef02b565f0a0c954297f38c4997c61b/pytorch_metric_learning-0.9.99-py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.0.1)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-0.9.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thprZ-b454a3",
        "outputId": "d2c0a5ae-495d-4e73-c5fe-e8bb5db96c8e"
      },
      "source": [
        "! git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --disable-pip-version-check --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8054, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 8054 (delta 68), reused 97 (delta 44), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8054/8054), 14.11 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (5469/5469), done.\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-gy1efw93\n",
            "Created temporary directory: /tmp/pip-req-tracker-n57sf3mw\n",
            "Created requirements tracker '/tmp/pip-req-tracker-n57sf3mw'\n",
            "Created temporary directory: /tmp/pip-install-equ213so\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-rph50oqh\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-n57sf3mw'\n",
            "    Running setup.py (path:/tmp/pip-req-build-rph50oqh/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-req-build-rph50oqh/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-rph50oqh/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-rph50oqh has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-n57sf3mw'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-lhcyqe8n\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-lhcyqe8n\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-rph50oqh/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-rph50oqh/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-lhcyqe8n --python-tag cp37\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.9.0+cu102\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-rph50oqh/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-lhcyqe8n/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204709 sha256=2813aed47193ce195c26a21174b5777296d2b4db6fe5111a5bf810bd60e786f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gy1efw93/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-rph50oqh\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-n57sf3mw'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PpkjSn24Kmv",
        "outputId": "f10bdbfa-31da-4585-e23e-0a4a6ddde83f"
      },
      "source": [
        "! cd ERICA/pretrain/code/pretrain; python -m torch.distributed.launch --nproc_per_node 8  main.py  \\\n",
        "    --model DOC  --lr 3e-5 --batch_size_per_gpu 16 --max_epoch 105  \\\n",
        "    --gradient_accumulation_steps 16    --save_step 500  --temperature 0.05  \\\n",
        "    --train_sample  --save_dir ckpt_doc_dw_f_alpha_1_uncased --n_gpu 8  --debug 1  --add_none 1 \\\n",
        "    --alpha 1 --flow 0 --dataset_name none.json  --wiki_loss 1 --doc_loss 1 \\\n",
        "    --change_dataset 1  --start_end_token 0 --bert_model bert \\\n",
        "    --pretraining_size -1 --ablation 0 --cased 0 --cuda 0 --local_rank 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  \"The module torch.distributed.launch is deprecated \"\n",
            "The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.\n",
            " Please read local_rank from `os.environ('LOCAL_RANK')` instead.\n",
            "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n",
            "  entrypoint       : main.py\n",
            "  min_nodes        : 1\n",
            "  max_nodes        : 1\n",
            "  nproc_per_node   : 8\n",
            "  run_id           : none\n",
            "  rdzv_backend     : static\n",
            "  rdzv_endpoint    : 127.0.0.1:29500\n",
            "  rdzv_configs     : {'rank': 0, 'timeout': 900}\n",
            "  max_restarts     : 3\n",
            "  monitor_interval : 5\n",
            "  log_dir          : None\n",
            "  metrics_cfg      : {}\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_y85v9iub/none_mscl3_fo\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.\n",
            "  \"This is an experimental API and will be changed in future.\", FutureWarning\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=0\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
            "  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/3/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/4/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/5/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/6/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_0/7/error.json\n",
            "2021-07-14 14:05:40.626550: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.627540: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.627932: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.628975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.631838: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.722730: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.730857: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:05:40.736128: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "\n",
            "\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 228, in <module>\n",
            "    os.mkdir(\"../../log\")\n",
            "FileExistsError: [Errno 17] File exists: '../../log'\n",
            "preparing data\n",
            "preparing data\n",
            "preparing data\n",
            "preparing data\n",
            "preparing data\n",
            "preparing data\n",
            "preparing data\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 240, in <module>\n",
            "    train_dataset = CP_R_Dataset(\"../../data/DOC\", args)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/dataset.py\", line 36, in __init__\n",
            "    self.entityMarker = EntityMarker(args)\n",
            "  File \"../utils.py\", line 36, in __init__\n",
            "    self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 375, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/content/ERICA/pretrain/code/pretrain/transformers/tokenization_utils.py\", line 476, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name '***path_to_your_bert_tokenizer_uncased***' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed '***path_to_your_bert_tokenizer_uncased***' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 5 (pid: 250) of binary: /usr/bin/python3\n",
            "ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n",
            "  restart_count=1\n",
            "  master_addr=127.0.0.1\n",
            "  master_port=29500\n",
            "  group_rank=0\n",
            "  group_world_size=1\n",
            "  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\n",
            "  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
            "  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]\n",
            "\n",
            "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/0/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/1/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/2/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/3/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/4/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/5/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/6/error.json\n",
            "INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /tmp/torchelastic_y85v9iub/none_mscl3_fo/attempt_1/7/error.json\n",
            "2021-07-14 14:06:11.423959: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.425274: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.428862: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.441007: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.442118: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.567516: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.571361: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 14:06:11.664989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Namespace(ablation=0.0, adam_epsilon=1e-08, add_none=1, alpha=1.0, bag_size=2, batch_size_per_gpu=16, bert_model='bert', cased=0.0, change_dataset=1.0, cuda='0', curve_step=100, dataset_name='none.json', debug=1, doc_loss=1.0, flow=0, gradient_accumulation_steps=16, hidden_size=768, local_rank=0, log_step=5, log_step_test=50, lr=3e-05, max_epoch=105, max_grad_norm=1, max_length=512, model='DOC', n_gpu=8, neg_sample_num=64, pretraining_size=-1, save_dir='ckpt_doc_dw_f_alpha_1_uncased', save_step=500, seed=42, start_end_token=0.0, temperature=0.05, train_sample=True, warmup_steps=500, weight_decay=1e-05, wiki_loss=1.0)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 221, in <module>\n",
            "  File \"main.py\", line 221, in <module>\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")Traceback (most recent call last):\n",
            "  File \"main.py\", line 221, in <module>\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 221, in <module>\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "    Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 221, in <module>\n",
            "      File \"main.py\", line 221, in <module>\n",
            "_store_based_barrier(rank, store, timeout)\n",
            "      File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 208, in _store_based_barrier\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "    worker_count = store.add(store_key, 0)\n",
            "KeyboardInterrupt\n",
            "    _store_based_barrier(rank, store, timeout)\n",
            "torch.distributed.init_process_group(backend=\"nccl\")  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 207, in _store_based_barrier\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "    time.sleep(0.01)\n",
            "KeyboardInterrupt\n",
            "    _store_based_barrier(rank, store, timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 208, in _store_based_barrier\n",
            "    _store_based_barrier(rank, store, timeout)worker_count = store.add(store_key, 0)\n",
            "KeyboardInterrupt\n",
            "\n",
            "      File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 208, in _store_based_barrier\n",
            "    _store_based_barrier(rank, store, timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 208, in _store_based_barrier\n",
            "    worker_count = store.add(store_key, 0)\n",
            "KeyboardInterrupt\n",
            "worker_count = store.add(store_key, 0)\n",
            "KeyboardInterrupt\n",
            "    _store_based_barrier(rank, store, timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 207, in _store_based_barrier\n",
            "    time.sleep(0.01)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 221, in <module>\n",
            "    torch.distributed.init_process_group(backend=\"nccl\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 547, in init_process_group\n",
            "    _store_based_barrier(rank, store, timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py\", line 207, in _store_based_barrier\n",
            "    time.sleep(0.01)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 173, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 169, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py\", line 624, in run\n",
            "    )(*cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 116, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 238, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 700, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 828, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwX24cmb-egA",
        "outputId": "aff5b98f-e4a4-425d-e5ae-9ad40a4d4d3b"
      },
      "source": [
        "import torch\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}